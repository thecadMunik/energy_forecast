{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ENERGY REAL-TIME FORECASTING PROJECT"
      ],
      "metadata": {
        "id": "yr6aq3j5d65b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages in Colab\n",
        "!pip install pandas  matplotlib seaborn scikit-learn xgboost requests"
      ],
      "metadata": {
        "id": "Q4BtUmcp0TLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading from URL SupaBase\n",
        "\n",
        "1. **Imports**  \n",
        "   - `pandas` for handling tables of data.  \n",
        "   - `requests` for making HTTP calls to the Supabase API.\n",
        "\n",
        "2. **Configuration**  \n",
        "   - `SUPABASE_URL` and `SUPABASE_API_KEY` point to your database and authorize access.  \n",
        "   - `TABLE` is the name of the table you’ll pull (`smart_meter_readings`).\n",
        "\n",
        "3. **`load_data()` function**  \n",
        "   - Builds the URL to fetch **all columns** (`select=*`) from your table, sorted by `timestamp` ascending (`order=timestamp.asc`).  \n",
        "   - Sends a GET request with your API key.  \n",
        "   - If successful (`status_code == 200`), prints the number of records retrieved and converts the JSON response into a pandas DataFrame.  \n",
        "   - If unsuccessful, raises an error with the status code and message.\n",
        "\n",
        "4. **Usage**  \n",
        "   - Calls `load_data()` to populate `df` with the full dataset.  \n",
        "   - Displays the first few rows of `df` using `df.head()`.  \n",
        "\n",
        "> **Purpose:** Pull the entire smart-meter readings table into your notebook for analysis."
      ],
      "metadata": {
        "id": "iwi18MFAfEmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Inspection"
      ],
      "metadata": {
        "id": "JqzWh3QPgRJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "SUPABASE_URL     = userdata.get('SupabaseURL')\n",
        "SUPABASE_API_KEY = userdata.get('SupabaseAPI')\n",
        "TABLE            = \"smart_meter_readings_1year\"\n",
        "\n",
        "def load_data():\n",
        "    url = (\n",
        "        f\"{SUPABASE_URL}/rest/v1/{TABLE}\"\n",
        "        \"?select=*\"               # fetch all columns\n",
        "        \"&order=timestamp.asc\"    # order by timestamp ascending\n",
        "    )\n",
        "    headers = {\n",
        "        \"apikey\": SUPABASE_API_KEY,\n",
        "        \"Authorization\": f\"Bearer {SUPABASE_API_KEY}\"\n",
        "    }\n",
        "\n",
        "    res = requests.get(url, headers=headers)\n",
        "    if res.status_code == 200:\n",
        "        print(\"✅ Data pulled successfully: \", len(res.json()), \"records\\n\")\n",
        "        return pd.DataFrame(res.json())\n",
        "    else:\n",
        "        raise Exception(f\"❌ Error: {res.status_code}\\n{res.text}\")\n",
        "\n",
        "# usage\n",
        "df = load_data()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5P19vypYn_53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "7ULuiSDnflJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 17431 rows and 13 columns in the dataset"
      ],
      "metadata": {
        "id": "LvyhqgCmgxbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking out the column names\n",
        "\n",
        "for col in df.columns:\n",
        "    print(col)"
      ],
      "metadata": {
        "id": "rFh-PExAgZQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at the various datatypes for each features\n",
        "df.info()"
      ],
      "metadata": {
        "id": "J5-gAVjYg4Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "VkAStH8ohoyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Convert the 'timestamp' column from Unix seconds to a readable datetime format and store it in a new 'datetime' column\n",
        "\n",
        "df['datetime'] = pd.to_datetime(df['timestamp'])"
      ],
      "metadata": {
        "id": "9Z5Ni4VWhaas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract date and ISO week number and store in new columns\n",
        "\n",
        "df['date'] = df['datetime'].dt.date\n",
        "df['week'] = df['datetime'].dt.isocalendar().week"
      ],
      "metadata": {
        "id": "eo2n82oghyhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Convert date to datetime, week to integer\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['week'] = df['week'].astype(int)"
      ],
      "metadata": {
        "id": "jjSizQRHkU_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6SS3b_Rni2Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "IBMRUVA7i9oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the date range and the number of days and unique ISO weeks in the dataset\n",
        "min_date = df['date'].min()\n",
        "max_date = df['date'].max()\n",
        "days     = (max_date - min_date).days + 1\n",
        "weeks    = df['week'].nunique()\n",
        "\n",
        "print(f\"Dataset covers {days} days, from {min_date} to {max_date} → {weeks} ISO weeks\\n\")\n"
      ],
      "metadata": {
        "id": "AAY3DWfUjFrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of readings for each date and display them sorted by date\n",
        "daily_counts = df['date'].value_counts().sort_index()\n",
        "print(\"Readings per day:\\n\", daily_counts)"
      ],
      "metadata": {
        "id": "iEmeIhjkjiBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Check for nulls and duplicates\n",
        "print(\"Nulls per column:\\n\", df.isnull().sum(), \"\\n\")\n",
        "print(\"Duplicates:\", df.duplicated().sum())"
      ],
      "metadata": {
        "id": "R8PyAXRQj91l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's no null or duplicate values in the dataset"
      ],
      "metadata": {
        "id": "KGyx2ItYkzdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Drop irrelevant columns\n",
        "df = df.drop(columns=['id', 'timestamp'])"
      ],
      "metadata": {
        "id": "sLoeg1Vhnp9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move 'datetime' to the index which automatically sorts the data in datetime order for time series analysis\n",
        "df = df.set_index('datetime').sort_index()"
      ],
      "metadata": {
        "id": "hZPrbK3coa-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Hdf2S8EloNpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round down all datetime index values to the nearest second\n",
        "df.index = df.index.floor('s')\n"
      ],
      "metadata": {
        "id": "i8wwxgxfoV4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "9oIMGzLXopqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization and Analysis"
      ],
      "metadata": {
        "id": "0pUWThi0oy0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Univariate Distribution"
      ],
      "metadata": {
        "id": "BwXY_sbd4wfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gSpKECRCosRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create histograms for all numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols].hist(figsize=(15, 20), bins=30, edgecolor='black', layout=(len(numeric_cols)//2 + 1, 2))\n",
        "\n",
        "# Get the current figure and axes\n",
        "fig = plt.gcf()\n",
        "axes = fig.get_axes()\n",
        "\n",
        "# Loop through each numeric column and corresponding subplot\n",
        "for ax, col in zip(axes, numeric_cols):\n",
        "    data = df[col].dropna()\n",
        "    mean = data.mean()\n",
        "    median = data.median()\n",
        "    mode = data.mode().iloc[0] if not data.mode().empty else np.nan\n",
        "    ax.axvline(mean, color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "    ax.axvline(median, color='green', linestyle='-', linewidth=2, label='Median')\n",
        "    ax.axvline(mode, color='orange', linestyle='-', linewidth=2, label='Mode')\n",
        "    ax.set_xlabel(col, fontsize=10)\n",
        "    ax.set_ylabel('Count', fontsize=10)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Distributions of the Numeric Data\", fontsize=16, y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vRMXFyho450j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations and Insights\n",
        "\n",
        "### 1. **meter_id**\n",
        "- **Metrics:** Range: 1000–1100; Mean ≈ 1050; Median ≈ 1050; Mode ≈ 1045. Counts for each meter are nearly uniform.\n",
        "- **What this means:**  \n",
        "  The dataset is well-balanced across all meters, indicating even data collection. This minimizes the risk of bias from any single location and ensures that insights or models apply generally across all metered sites.\n",
        "\n",
        "### 2. **power_consumption_kwh**\n",
        "- **Metrics:** Range: ~0.1–7.5 kWh per interval; Mean ≈ 1.3 kWh; Median ≈ 0.5 kWh; Mode ≈ 0.1 kWh. Over 60% of intervals are below 1 kWh; distribution is strongly right-skewed.\n",
        "- **What this means:**  \n",
        "  Most intervals reflect modest energy use, but occasional high-usage intervals drive the average upward. This is typical for large energy datasets, where normal daily activity is punctuated by spikes (from heavy appliances, EV charging, or industrial equipment).\n",
        "\n",
        "### 3. **voltage**\n",
        "- **Metrics:** Range: 224–231 V; Mean ≈ 228.7 V; Median ≈ 229 V; Mode: 229 V. About 90% of readings fall between 227 V and 231 V.\n",
        "- **What this means:**  \n",
        "  Voltage is tightly regulated and stable for most of the year, indicating a reliable power grid. The few lower readings likely represent minor dips or simulation noise, not systemic supply issues.\n",
        "\n",
        "### 4. **current**\n",
        "- **Metrics:** Range: 0.1–22 A; Mean ≈ 5.2 A; Median ≈ 2.4 A; Mode ≈ 0.7 A. Over 75% of readings are below 5 A; highly right-skewed distribution.\n",
        "- **What this means:**  \n",
        "  The vast majority of intervals have low current draw, with infrequent but significant spikes. These higher values likely reflect periods of peak demand or operation of large appliances.\n",
        "\n",
        "### 5. **temperature_c**\n",
        "- **Metrics:** Range: 4–35°C; Mean ≈ 20°C; Median ≈ 20°C; Mode ≈ 17°C. Most temperatures are between 12°C and 28°C, with a mild central peak.\n",
        "- **What this means:**  \n",
        "  The temperature data captures a full annual cycle, covering both warm and cool periods. The broad, nearly symmetrical distribution supports seasonality analysis.\n",
        "\n",
        "### 6. **humidity_pct**\n",
        "- **Metrics:** Range: 52–87%; Mean ≈ 69%; Median ≈ 70%; Mode ≈ 83%. Humidity is broadly distributed with two main peaks (around 58% and 83%).\n",
        "- **What this means:**  \n",
        "  Humidity reflects strong seasonality, likely corresponding to dry and rainy periods. This variation enables robust analysis of how weather affects power consumption.\n",
        "\n",
        "### 7. **hour_of_day**\n",
        "- **Metrics:** Range: 0–23; Mean ≈ 11.5; all hours are equally represented.\n",
        "- **What this means:**  \n",
        "  There is excellent coverage across every hour, ensuring that time-of-day patterns can be analyzed without bias or missing intervals.\n",
        "\n",
        "### 8. **week**\n",
        "- **Metrics:** Range: 1–52; all weeks in the year are covered and equally represented.\n",
        "- **What this means:**  \n",
        "  The data spans a full year, supporting both seasonal and long-term trend analysis. No weeks are missing, so time-based insights will be robust."
      ],
      "metadata": {
        "id": "Li-rpAqNtD3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create barplots that indicate percentage for each category.\n",
        "def bar_perc(plot, feature):\n",
        "    total = len(feature) # length of the column\n",
        "    for p in plot.patches:\n",
        "        percentage = '{:.1f}%'.format(100 * p.get_height()/total) # percentage of each class of the category\n",
        "        x = p.get_x() + p.get_width() / 2 - 0.05 # width of the plot\n",
        "        y = p.get_y() + p.get_height()           # height of the plot\n",
        "        plot.annotate(percentage, (x, y), size = 12) # annotate the percentage"
      ],
      "metadata": {
        "id": "Z7jW6QRWp_r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get all category datatype\n",
        "list_col=  df.select_dtypes(['object', 'bool']).columns\n",
        "\n",
        "fig1, axes1 =plt.subplots(2,2,figsize=(10, 8))\n",
        "axes1 = axes1.flatten()\n",
        "for i in range(len(list_col)):\n",
        "    order = df[list_col[i]].value_counts(ascending=False).index # to display bar in ascending order\n",
        "    axis=sns.countplot(x=list_col[i], data=df , order=order,ax=axes1[i],palette='coolwarm').set(title=list_col[i])\n",
        "    bar_perc(axes1[i],df[list_col[i]])\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(len(list_col), len(axes1)):\n",
        "    fig1.delaxes(axes1[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Distributions of the Categorical Data\", fontsize=16, y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A8pvgvkCtYzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical Variable Distributions\n",
        "\n",
        "#### 1. Region\n",
        "**Metrics:**  \n",
        "South: 29.7% | West: 25.3% | East: 25.2% | North: 19.8%\n",
        "\n",
        "**What this means:**  \n",
        "The dataset is relatively well-distributed across all regions, but the **South** has the highest representation (nearly 30%), while the **North** has the lowest (under 20%). This may reflect larger population centers, more meters installed, or higher data collection frequency in the South.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Property Type\n",
        "**Metrics:**  \n",
        "Residential: 50.6% | Commercial: 49.4%\n",
        "\n",
        "**What this means:**  \n",
        "The split between **residential** and **commercial** properties is almost perfectly balanced. This is ideal for comparative analysis and ensures there is no inherent bias toward one property type.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. EV Owner\n",
        "**Metrics:**  \n",
        "False: 50.2% | True: 49.8%\n",
        "\n",
        "**What this means:**  \n",
        "**EV ownership** is very evenly distributed, allowing for robust comparisons of power consumption patterns between EV owners and non-owners. This balanced distribution supports fair modeling and analysis.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Solar Installed\n",
        "**Metrics:**  \n",
        "False: 70.0% | True: 30.0%\n",
        "\n",
        "**What this means:**  \n",
        "A significant majority of properties **do not have solar panels** (70%), while 30% do. This gives good statistical power for comparing solar versus non-solar homes, though findings for the solar group may be less robust for rare patterns."
      ],
      "metadata": {
        "id": "XyN5p5y0ucAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate Distribution"
      ],
      "metadata": {
        "id": "Txc7GsdjusOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Power Consumption by Region, Property Type, EV Owner and Solar Installed\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 7))\n",
        "\n",
        "# 1. Region\n",
        "sns.boxplot(x='region', y='power_consumption_kwh', data=df, ax=axes[0,0])\n",
        "axes[0,0].set_title(\"Power Consumption by Region\")\n",
        "\n",
        "# 2. Property Type\n",
        "sns.boxplot(x='property_type', y='power_consumption_kwh', data=df, ax=axes[0,1])\n",
        "axes[0,1].set_title(\"Power Consumption by Property_Type\")\n",
        "\n",
        "# 3. EV Owner\n",
        "sns.boxplot(x='ev_owner', y='power_consumption_kwh', data=df, ax=axes[1,0])\n",
        "axes[1,0].set_title(\"Power Consumption by EV_Owner\")\n",
        "\n",
        "# 4. Solar Installed\n",
        "sns.boxplot(x='solar_installed', y='power_consumption_kwh', data=df, ax=axes[1,1])\n",
        "axes[1,1].set_title(\"Power Consumption by Solar_Installed\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YsQUq_g5teLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boxplot Observations: Power Consumption\n",
        "\n",
        "#### 1. Region\n",
        "- **West & East:** Highest medians (~1.6 kWh) and largest upper whiskers (peaks up to ~5 kWh), pointing to heavier loads or more frequent use of large appliances in these areas.\n",
        "- **South:** Similar distribution to West/East, but with slightly fewer extreme high intervals (upper limit around 4.5 kWh).\n",
        "- **North:** Lower median (~1.3 kWh) and a more compact interquartile range, indicating lighter and more consistent usage.\n",
        "- **What this means:** Power usage per interval is highest in the West and East, and lowest in the North.\n",
        "\n",
        "#### 2. Property Type\n",
        "- **Residential:** Higher median (~1.1 kWh) and wider spread, with more outliers—reflects more diverse and generally higher usage.\n",
        "- **Commercial:** Lower median (~0.7 kWh) and a tighter distribution, with fewer outliers and less variability.\n",
        "- **What this means:** Residential properties are the primary drivers of higher and more variable interval power consumption.\n",
        "\n",
        "#### 3. EV Owner\n",
        "- **Both groups:** Nearly identical boxplots, with medians around 0.9 kWh, similar spreads, and similar maximum values.\n",
        "- **What this means:** EV ownership does not significantly impact overall interval power consumption in this dataset.\n",
        "\n",
        "#### 4. Solar Installed\n",
        "- **Both groups:** Medians and distribution shapes are almost identical (~0.9 kWh), showing no visible impact from solar installation on recorded interval consumption.\n",
        "- **What this means:** Having solar panels does not noticeably alter overall usage patterns in this sample.\n",
        "\n",
        "**General Remark:**  \n",
        "The main drivers of variation in power consumption intervals are property type and region, not EV ownership or solar installation."
      ],
      "metadata": {
        "id": "bbSoZcPZvPcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily & Weekly Time‐Series Patterns\n",
        "\n",
        "# Resample to daily mean consumption\n",
        "daily = df['power_consumption_kwh'].resample('D').mean()\n",
        "plt.figure(figsize=(10,3))\n",
        "daily.plot()\n",
        "plt.title(\"Daily Average Power Consumption\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.show()\n",
        "\n",
        "# Hour‐of‐day profile (averaged across all days)\n",
        "hourly = df.groupby('hour_of_day')['power_consumption_kwh'].mean()\n",
        "plt.figure(figsize=(8,3))\n",
        "hourly.plot(marker='o')\n",
        "plt.title(\"Average Consumption by Hour of Day\")\n",
        "plt.xlabel(\"Hour (0–23)\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xticks(range(0,24,2))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ksjUu2NvxuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Power consumption per month\n",
        "# 1. Resample to monthly total consumption\n",
        "monthly = df['power_consumption_kwh'].resample('M').sum()\n",
        "\n",
        "# 2. Create month labels (e.g., 'Jul 2025')\n",
        "month_labels = monthly.index.strftime('%b %Y')\n",
        "\n",
        "# 3. Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(month_labels, monthly.values, marker='o')\n",
        "plt.title(\"Total Power Consumption by Month\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total kWh\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P18jfCGE2okI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations: Total Power Consumption by Month\n",
        "\n",
        "- **Highest Consumption:** Power usage peaks in **March 2026**, with total consumption just above 1900 kWh, closely followed by October 2025, January 2026, and the months of spring.\n",
        "- **Lowest Consumption:** The lowest total is seen in **July 2026** (about 600 kWh), which is likely due to incomplete data for that month.\n",
        "- **Consistent Pattern:** From August 2025 through June 2026, monthly usage remains fairly stable, ranging between 1750 and 1920 kWh.\n",
        "- **Seasonal Insight:** The consistently high consumption throughout late summer, autumn, winter, and spring suggests year-round demand, possibly driven by both heating (in colder months) and cooling (in warmer months), or steady activity across all months.\n",
        "- **Note:** Partial data in the first and last July periods can make these months appear artificially low.\n"
      ],
      "metadata": {
        "id": "lKP8J2T6286F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create a 'month' column for grouping\n",
        "df['month'] = df.index.to_period('M').to_timestamp()\n",
        "\n",
        "# 2. Group by month and property type, then sum power consumption\n",
        "monthly_by_type = (\n",
        "    df.groupby(['month', 'property_type'])['power_consumption_kwh']\n",
        "    .sum()\n",
        "    .unstack()  # property_type becomes columns\n",
        ")\n",
        "\n",
        "# 3. Plot each property type as a separate line\n",
        "plt.figure(figsize=(12,5))\n",
        "for col in monthly_by_type.columns:\n",
        "    plt.plot(monthly_by_type.index, monthly_by_type[col], marker='o', label=col.capitalize())\n",
        "\n",
        "plt.title(\"Total Power Consumption by Month and Property Type\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total kWh\")\n",
        "plt.xticks(monthly_by_type.index, [d.strftime('%b %Y') for d in monthly_by_type.index], rotation=45, ha='right')\n",
        "plt.legend(title='Property Type')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rcsb7Daw3b3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations: Monthly Total Power Consumption by Property Type\n",
        "\n",
        "- **Residential properties** consistently consume more electricity each month than commercial properties. Residential usage typically ranges from about **950 to 1,100 kWh** per month, while commercial properties range between **700 and 900 kWh**.\n",
        "- Both types follow a similar seasonal pattern, with higher usage in **late summer and early autumn** (August to October), and somewhat lower totals during the winter months (especially February).\n",
        "- **Residential consumption** reaches its peak in **August** and again in **May–June**, likely due to increased heating or cooling needs during these periods.\n",
        "- The lowest recorded consumption appears in **July 2026** for both categories, which is probably due to incomplete data for that month.\n",
        "- **Commercial consumption** is slightly more variable from month to month, which may reflect fluctuations in business activity or the influence of holidays.\n",
        "\n",
        "Overall, residential buildings are the main contributors to annual energy use, but both groups show similar rises and falls throughout the year, suggesting that seasonal factors impact both property types in parallel.\n"
      ],
      "metadata": {
        "id": "YHqZEarS38XL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Weekly Average Power Consumption (better for yearly data) ---\n",
        "# Aggregate to weekly mean\n",
        "weekly = df['power_consumption_kwh'].resample('W').mean()\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(weekly.index, weekly.values, marker='o')\n",
        "plt.title(\"Weekly Average Power Consumption\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xlabel(\"Week\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 2. Average Consumption by Day of Week (cycle) ---\n",
        "# If your index is a datetime, extract day name\n",
        "df['weekday'] = df.index.day_name()  # Use df['ds'].dt.day_name() if your index is not datetime\n",
        "\n",
        "# Get mean for each day of the week\n",
        "dow_avg = df.groupby('weekday')['power_consumption_kwh'].mean()\n",
        "# Ensure proper order: Monday to Sunday\n",
        "dow_avg = dow_avg.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(dow_avg.index, dow_avg.values, marker='o')\n",
        "plt.title(\"Average Power Consumption by Day of Week\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xlabel(\"Day of Week\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 3. Hourly Average Consumption (remains unchanged) ---\n",
        "hourly = df.groupby('hour_of_day')['power_consumption_kwh'].mean()\n",
        "hours = hourly.index\n",
        "time_labels = [f\"{h:02d}:00\" for h in hours]\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(hours, hourly.values, marker='o')\n",
        "plt.title(\"Average Consumption by Time of Day\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xticks(hours, time_labels, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ObvrrivWx_Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations: Weekly Average Power Consumption\n",
        "\n",
        "- **Fluctuations:** Weekly average power consumption varies between about **1.1 kWh and 1.4 kWh**, showing regular rises and dips throughout the year.\n",
        "- **Short-Term Peaks:** Repeating peaks occur every few weeks, likely linked to operational or environmental factors such as weather changes or holidays.\n",
        "- **No Major Outliers:** There are no extreme spikes or drops, suggesting overall stable demand.\n",
        "- **Real-life Context:** These trends reflect a mix of business cycles, weather, and seasonal influences on energy use.\n",
        "\n",
        "---\n",
        "\n",
        "### Observations: Average Power Consumption by Day of Week\n",
        "\n",
        "- **Workweek Higher:** Energy use is **highest Monday to Friday** (roughly **1.35–1.42 kWh** daily), peaking on Fridays.\n",
        "- **Weekend Drop:** There’s a clear decrease on **Saturdays and Sundays** (about **0.95–1.0 kWh**), indicating less activity.\n",
        "- **What This Means:** This pattern aligns with typical routines—higher weekday demand due to work, school, and business; lower on weekends when more people may be out or businesses closed.\n",
        "\n",
        "---\n",
        "\n",
        "### Observations: Average Consumption by Time of Day\n",
        "\n",
        "- **Nighttime Low:** Power use is lowest between **midnight and 6 AM** (~**0.4 kWh**), reflecting minimal activity.\n",
        "- **Morning Ramp-up:** Usage climbs rapidly from **7 AM** onward, especially between **8–10 AM**.\n",
        "- **Afternoon Peak:** The highest consumption occurs between **10 AM and 4 PM** (up to **2.2 kWh**), coinciding with business hours and daytime routines.\n",
        "- **Evening Decline:** After **4 PM**, power use gradually decreases, stabilizing in the evening and dropping after **10 PM**.\n",
        "- **What This Means:** This daily pattern matches real-world behavior—low overnight, sharp morning rise, sustained daytime peak, then evening wind-down.\n"
      ],
      "metadata": {
        "id": "bCn0KsHFzA1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weekly consumption by Propert Type\n",
        "\n",
        "# 1. Compute weekly mean consumption for each type\n",
        "weekly_type = (\n",
        "    df\n",
        "    .groupby([pd.Grouper(freq='W'), 'property_type'])['power_consumption_kwh']\n",
        "    .mean()\n",
        "    .unstack()  # columns = residential, commercial\n",
        ")\n",
        "\n",
        "# 2. Generate readable x-labels (week starting date)\n",
        "week_labels = weekly_type.index.strftime('Week of %d %b')\n",
        "\n",
        "# 3. Plot on two subplots\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "for ax, ptype in zip(axes, ['residential', 'commercial']):\n",
        "    ax.plot(weekly_type.index, weekly_type[ptype], marker='o')\n",
        "    ax.set_title(f\"Weekly Average Power Consumption: {ptype.capitalize()}\")\n",
        "    ax.set_ylabel(\"kWh\")\n",
        "    ax.grid(True)\n",
        "\n",
        "# 4. Customize x-ticks on bottom subplot\n",
        "axes[-1].set_xticks(weekly_type.index)\n",
        "axes[-1].set_xticklabels(week_labels, rotation=45, ha='right')\n",
        "axes[-1].set_xlabel(\"Week\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Hourly Average by Property Type (unchanged)\n",
        "hourly_type = (\n",
        "    df\n",
        "    .groupby(['hour_of_day', 'property_type'])['power_consumption_kwh']\n",
        "    .mean()\n",
        "    .unstack()\n",
        ")\n",
        "\n",
        "time_labels = [f\"{h:02d}:00\" for h in hourly_type.index]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for ptype in ['residential', 'commercial']:\n",
        "    plt.plot(hourly_type.index, hourly_type[ptype], marker='o', label=ptype.capitalize())\n",
        "\n",
        "plt.title(\"Average Consumption by Hour of Day\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xticks(hourly_type.index, time_labels, rotation=45, ha='right')\n",
        "plt.legend(title=\"Property Type\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LU3724tc08IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weekly Average Power Consumption: Residential\n",
        "\n",
        "- **Residential power usage** remains higher than commercial throughout the year, typically ranging from **1.3 to 1.5 kWh per week**.\n",
        "- Fluctuations are mild, with most weeks staying within a narrow band, indicating stable household routines.\n",
        "- **Interpretation:** Residential energy demand is steady, likely due to consistent appliance use and regular living patterns, with occasional variations due to holidays, weather, or other seasonal effects.\n",
        "\n",
        "---\n",
        "\n",
        "### Weekly Average Power Consumption: Commercial\n",
        "\n",
        "- **Commercial usage** is generally lower, averaging **0.9–1.2 kWh per week**, but with a few sharper weekly peaks approaching **1.5 kWh**.\n",
        "- There is slightly more variability, with occasional spikes likely reflecting special business activities or seasonality.\n",
        "- **Interpretation:** Commercial properties show increased energy use during certain periods, possibly linked to events or operational peaks, while maintaining lower consumption during routine weeks or holidays.\n",
        "\n",
        "---\n",
        "\n",
        "### Average Consumption by Hour of Day (Property Type Comparison)\n",
        "\n",
        "- **Residential:** Power usage increases after **6 AM**, stays elevated between **7 AM and 9 PM** (about **1.5–2.0 kWh**), and drops overnight (to around **0.5 kWh**).\n",
        "- **Commercial:** Remains low overnight (**0.3 kWh**), rises rapidly after **8 AM**, peaks between **1 PM and 4 PM** (**up to 3.0 kWh**), and then falls sharply after business hours.\n",
        "- **Interpretation:** Residential consumption follows typical daily routines, while commercial usage is highly concentrated during standard business hours, reflecting operational schedules.\n",
        "\n"
      ],
      "metadata": {
        "id": "woAhboii2Mg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weekly consumption by Ev Owner\n",
        "# 1. Compute weekly mean consumption for each EV owner group\n",
        "weekly_ev = (\n",
        "    df\n",
        "    .groupby([pd.Grouper(freq='W'), 'ev_owner'])['power_consumption_kwh']\n",
        "    .mean()\n",
        "    .unstack()  # columns = True, False\n",
        ")\n",
        "\n",
        "# 2. Create readable week labels\n",
        "week_labels = [\"Week of \" + d.strftime('%d %b') for d in weekly_ev.index]\n",
        "\n",
        "# 3. Plot on two subplots (one for EV owners, one for non-owners)\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
        "for ax, owner in zip(axes, [True, False]):\n",
        "    ax.plot(weekly_ev.index, weekly_ev[owner], marker='o')\n",
        "    label = \"EV Owner\" if owner else \"No EV\"\n",
        "    ax.set_title(f\"Weekly Average Power Consumption: {label}\")\n",
        "    ax.set_ylabel(\"kWh\")\n",
        "    ax.grid(True)\n",
        "\n",
        "# 4. Set week labels on x-axis\n",
        "axes[-1].set_xticks(weekly_ev.index)\n",
        "axes[-1].set_xticklabels(week_labels, rotation=45, ha='right')\n",
        "axes[-1].set_xlabel(\"Week\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Hourly average by EV owner\n",
        "hourly_ev = (\n",
        "    df\n",
        "    .groupby(['hour_of_day', 'ev_owner'])['power_consumption_kwh']\n",
        "    .mean()\n",
        "    .unstack()  # columns = True, False\n",
        ")\n",
        "time_labels = [f\"{h:02d}:00\" for h in hourly_ev.index]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for owner in [True, False]:\n",
        "    label = \"EV Owner\" if owner else \"No EV\"\n",
        "    plt.plot(hourly_ev.index, hourly_ev[owner], marker='o', label=label)\n",
        "plt.title(\"Average Consumption by Hour of Day (EV Owner)\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xticks(hourly_ev.index, time_labels, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VKqMUD_q2K27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weekly Average Power Consumption: EV Owner\n",
        "\n",
        "- **EV Owners:** Weekly average power consumption for EV owners generally ranges from **1.0 to 1.4 kWh**, with a gradual increase over the year and several noticeable peaks. Most values remain above 1.1 kWh, showing a consistently higher usage baseline.\n",
        "- **No EV:** Non-EV owners have slightly lower weekly averages, mostly between **1.1 and 1.3 kWh**, and their usage is more stable, with fewer sharp peaks compared to EV owners.\n",
        "- **Interpretation:** EV ownership is associated with a higher and more variable average weekly power consumption, likely due to charging needs that add to regular household usage.\n",
        "\n",
        "### Average Consumption by Hour of Day (EV Owner Comparison)\n",
        "\n",
        "- **Both Groups:** Power consumption patterns by hour are very similar for both EV owners and non-EV owners, with overnight lows (0.4–0.5 kWh), a morning rise from 6 AM, and prominent peaks between **10 AM and 4 PM** (about 2.0–2.3 kWh).\n",
        "- **Subtle Differences:** EV owners have slightly higher afternoon peaks, but the overall daily curves closely match.\n",
        "- **Interpretation:** Despite the higher weekly average for EV owners, hourly usage patterns remain largely parallel, suggesting that charging is distributed in a way that aligns with general household routines, rather than causing unique new peaks.\n"
      ],
      "metadata": {
        "id": "BG5Zz7dr5vWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Weekly average by Solar Installed\n",
        "\n",
        "# 1. weekly average\n",
        "weekly_solar = (\n",
        "    df\n",
        "    .groupby([pd.Grouper(freq='W-MON'), 'solar_installed'])['power_consumption_kwh']\n",
        "    .mean()\n",
        "    .unstack()  # columns = True, False\n",
        ")\n",
        "\n",
        "# 2. Generate readable week labels (e.g., \"Week of 13 Jul\")\n",
        "week_labels = [\"Week of \" + d.strftime('%d %b') for d in weekly_solar.index]\n",
        "\n",
        "# 3. Plot weekly averages in two subplots\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
        "for ax, solar in zip(axes, [True, False]):\n",
        "    ax.plot(weekly_solar.index, weekly_solar[solar], marker='o')\n",
        "    label = \"Solar Installed\" if solar else \"No Solar\"\n",
        "    ax.set_title(f\"Weekly Average Power Consumption: {label}\")\n",
        "    ax.set_ylabel(\"kWh\")\n",
        "    ax.grid(True)\n",
        "\n",
        "# X-axis labels for bottom subplot\n",
        "axes[-1].set_xticks(weekly_solar.index)\n",
        "axes[-1].set_xticklabels(week_labels, rotation=45, ha='right')\n",
        "axes[-1].set_xlabel(\"Week\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Hourly average by Solar Installed (remains unchanged)\n",
        "hourly_solar = (\n",
        "    df\n",
        "    .groupby(['hour_of_day', 'solar_installed'])['power_consumption_kwh']\n",
        "    .mean()\n",
        "    .unstack()  # columns = True, False\n",
        ")\n",
        "time_labels = [f\"{h:02d}:00\" for h in hourly_solar.index]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for solar in [True, False]:\n",
        "    label = \"Solar Installed\" if solar else \"No Solar\"\n",
        "    plt.plot(hourly_solar.index, hourly_solar[solar], marker='o', label=label)\n",
        "plt.title(\"Average Consumption by Hour of Day (Solar Installed)\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.xticks(hourly_solar.index, time_labels, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IOiXHaN763os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weekly Average Power Consumption: Solar Installed\n",
        "\n",
        "- **Solar Installed:** Properties with solar panels generally show a higher and more variable weekly power consumption, with peaks above 1.6 kWh and fluctuations throughout the year. The higher average could reflect larger or more energy-active households, or possible feedback to the grid.\n",
        "- **No Solar:** Properties without solar are more consistent, averaging around 1.2–1.4 kWh per week, with less dramatic swings and a slightly lower ceiling compared to the solar group.\n",
        "- **Interpretation:** Solar adoption is associated with higher observed power use, but the variation suggests that lifestyle, system size, or local conditions may also play a role.\n",
        "\n",
        "### Average Consumption by Hour of Day (Solar Installed)\n",
        "\n",
        "- **Solar Installed:** Average power use rises sharply after 6 AM, peaking between 2–4 PM (up to 2.2 kWh), then declines in the evening. This pattern could align with active daytime usage and possible feedback from solar systems during peak sun hours.\n",
        "- **No Solar:** Similar daily curve, but with a slightly lower afternoon peak and consistently lower consumption in most hours.\n",
        "- **Interpretation:** Both groups have similar time-of-day profiles, but the solar group consistently uses (or returns) more energy, especially during midday hours. This highlights the impact of solar adoption on both consumption and potential generation patterns.\n"
      ],
      "metadata": {
        "id": "W1c3LWYc7g5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Matrix"
      ],
      "metadata": {
        "id": "nSBBAWSu7wa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlations\n",
        "corr = df[['power_consumption_kwh','voltage','current','temperature_c','humidity_pct']].corr()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XLzRboe47CN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Power consumption** is perfectly positively correlated with **current** (1.00) and very strongly negatively correlated with **voltage** (-0.91). This is typical in electrical systems: as current increases for a given voltage, so does power usage, and vice versa.\n",
        "- **Humidity** shows a mild positive correlation with power consumption (0.22), while **temperature** is slightly negatively correlated (-0.19), suggesting that weather factors play only a small role in influencing energy use in this dataset.\n",
        "- **Temperature and humidity** are very strongly negatively correlated (-0.86), reflecting the common environmental pattern where higher temperatures often coincide with lower humidity.\n",
        "- **Voltage and current** are also very strongly negatively correlated (-0.91), further highlighting their inverse relationship in household or commercial power distribution.\n"
      ],
      "metadata": {
        "id": "bAi669YF8lS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling - Experimenting With Time Series Models: Prophet and SARIMAX\n",
        "\n",
        "In this section, we explore two powerful time series forecasting models:\n",
        "\n",
        "- **Prophet**: Developed by Facebook, Prophet is an open-source forecasting tool designed for time series data with strong seasonal effects and historical trends. It's robust to missing data, handles outliers well, and supports custom seasonality and holidays, making it ideal for business and economic forecasting.\n",
        "\n",
        "- **SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors)**: SARIMAX is a statistical model used to forecast time series data that may have trend and seasonality components. It extends the ARIMA model by including:\n",
        "  - **Seasonality**: Repeats over time (e.g., daily, weekly patterns)\n",
        "  - **Exogenous variables (X)**: External predictors that can improve forecast accuracy\n",
        "  - **AR (AutoRegressive)**, **I (Integrated)**, and **MA (Moving Average)** components to model different aspects of the time series' structure.\n",
        "\n",
        "We will use these models to evaluate and compare their performance on our dataset."
      ],
      "metadata": {
        "id": "AkrfPt1jHryt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PROPHET MODEL"
      ],
      "metadata": {
        "id": "21shJzALHuDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy to preserve the original\n",
        "df_prophet = df.copy()\n",
        "\n",
        "# Prophet needs 'ds' and 'y' columns\n",
        "df_prophet['ds'] = df_prophet.index\n",
        "df_prophet['y'] = df_prophet['power_consumption_kwh']\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "df_prophet['ev_owner'] = df_prophet['ev_owner'].astype(int)\n",
        "df_prophet['solar_installed'] = df_prophet['solar_installed'].astype(int)"
      ],
      "metadata": {
        "id": "Rf8rDN5O9Mek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prophet.head()"
      ],
      "metadata": {
        "id": "WElZ5rT9Hy2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This next step creates cyclical time features (hour_sin and hour_cos) from hour_of_day.\n",
        "Uses sine and cosine transformations to reflect the repeating daily cycle.\n",
        "This helps models recognize that hour 23 and hour 0 are close in time,\n",
        "but not in numeric value, improving the capture of daily patterns.\n",
        "\"\"\"\n",
        "# Create sine and cosine features from hour_of_day (0–23)\n",
        "df_prophet['hour_sin'] = np.sin(2 * np.pi * df_prophet['hour_of_day'] / 24)\n",
        "df_prophet['hour_cos'] = np.cos(2 * np.pi * df_prophet['hour_of_day'] / 24)"
      ],
      "metadata": {
        "id": "2BzNvisTH1fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode region and property_type\n",
        "region_dummies = pd.get_dummies(df_prophet['region'], prefix='region')\n",
        "property_dummies = pd.get_dummies(df_prophet['property_type'], prefix='property')\n",
        "\n",
        "# Add them to the main DataFrame\n",
        "df_prophet = pd.concat([df_prophet, region_dummies, property_dummies], axis=1)"
      ],
      "metadata": {
        "id": "j7gr4D6cH8B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = [\n",
        "    'ds', 'y',\n",
        "    'temperature_c', 'humidity_pct',\n",
        "    'ev_owner', 'solar_installed',\n",
        "    'hour_sin', 'hour_cos',\n",
        "    'region_east', 'region_north', 'region_south', 'region_west',\n",
        "    'property_commercial', 'property_residential'\n",
        "]\n",
        "\n",
        "df_prophet = df_prophet[selected_columns]"
      ],
      "metadata": {
        "id": "CvHreZUWIDEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort and split the Time Series Data\n",
        "# Sort by time\n",
        "df_prophet = df_prophet.sort_values('ds')\n",
        "\n",
        "# Split by 80%\n",
        "split_idx = int(len(df_prophet) * 0.8)\n",
        "train_df = df_prophet.iloc[:split_idx]\n",
        "test_df = df_prophet.iloc[split_idx:]"
      ],
      "metadata": {
        "id": "9SEIco6jIKF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "\n",
        "# Initialize the model\n",
        "model = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=False)\n",
        "\n",
        "# Add all regressors\n",
        "for regressor in selected_columns:\n",
        "    if regressor not in ['ds', 'y']:\n",
        "        model.add_regressor(regressor)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(train_df)"
      ],
      "metadata": {
        "id": "UHDev4vJIOvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove target column from test\n",
        "future_test = test_df.drop(columns=['y'])\n",
        "\n",
        "# Predict\n",
        "forecast = model.predict(future_test)"
      ],
      "metadata": {
        "id": "me8bqfZtIS3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Compute metrics\n",
        "mae = mean_absolute_error(test_df['y'], forecast['yhat'])\n",
        "rmse = np.sqrt(mean_squared_error(test_df['y'], forecast['yhat']))\n",
        "\n",
        "print(f\"MAE: {mae:.3f} kWh\")\n",
        "print(f\"RMSE: {rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "J0FtNU4XIZW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparing with Baseline models"
      ],
      "metadata": {
        "id": "MCE3566TIm50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing True and Predicted Values for Comparison\n",
        "\n",
        "- `y_true =test_df['y'].reset_index(drop=True)`: Extracts the actual target values from the results and resets the index to ensure alignment.\n",
        "\n",
        "- `y_pred_prophet = forecast['yhat'].reset_index(drop=True)`: Extracts the predicted values from Prophet (`'yhat'`) and also resets the index.\n",
        "\n",
        "Resetting the index ensures that both `y_true` and `y_pred_prophet` are properly aligned by position, which is important for plotting or calculating metrics like correlation or custom error analysis."
      ],
      "metadata": {
        "id": "9rtOrcq0I1ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = test_df['y'].reset_index(drop=True)\n",
        "y_pred_prophet = forecast['yhat'].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gHeZ-6WxIgp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Naïve Forecast Benchmark\n",
        "This cell creates a simple naïve forecast to serve as a baseline for evaluating Prophet's performance:\n",
        "\n",
        "y_pred_naive = y_true.shift(1): Assumes that the best prediction for the current time step is the actual value from 30 minutes ago (i.e., a 1-step lag).\n",
        "\n",
        "mean_absolute_error(y_true[1:], y_pred_naive[1:]): Calculates MAE between the actual and naïve predictions, excluding the first row (which becomes NaN after the shift).\n",
        "\n",
        "mean_squared_error(...): Computes RMSE the same way.\n",
        "\"\"\"\n",
        "\n",
        "# Shift the true values by 1 step (30 mins ago)\n",
        "y_pred_naive = y_true.shift(1)\n",
        "\n",
        "# Drop the first value to align\n",
        "naive_mae = mean_absolute_error(y_true[1:], y_pred_naive[1:])\n",
        "naive_rmse = np.sqrt(mean_squared_error(y_true[1:], y_pred_naive[1:]))\n",
        "\n",
        "print(f\"Naïve MAE: {naive_mae:.3f} kWh\")\n",
        "print(f\"Naïve RMSE: {naive_rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "CbaKauzEI2YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Seasonal Naïve Forecast Benchmark (1-Day Lag)\n",
        "This cell sets up a seasonal naïve model that assumes the power consumption at a given time is the same as exactly 24 hours earlier:\n",
        "\n",
        "y_true.shift(48): Shifts the actual values by 48 steps, assuming the data has 30-minute intervals (48 steps = 1 day).\n",
        "\n",
        "The forecast assumes today’s consumption will match the same time yesterday.\n",
        "\n",
        "mean_absolute_error(...) and mean_squared_error(...): Compute MAE and RMSE, skipping the first 48 rows to avoid misalignment due to shifting.\n",
        "\n",
        "This benchmark captures daily seasonality and helps assess whether Prophet's more advanced forecasting offers an improvement over this simple, seasonal assumption.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Shift by 48 steps for 1-day seasonal naive\n",
        "y_pred_seasonal_naive = y_true.shift(48)\n",
        "\n",
        "seasonal_mae = mean_absolute_error(y_true[48:], y_pred_seasonal_naive[48:])\n",
        "seasonal_rmse = np.sqrt(mean_squared_error(y_true[48:], y_pred_seasonal_naive[48:]))\n",
        "\n",
        "print(f\"Seasonal Naïve MAE: {seasonal_mae:.3f} kWh\")\n",
        "print(f\"Seasonal Naïve RMSE: {seasonal_rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "Y68BCqleI92O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_mae = mean_absolute_error(y_true, y_pred_prophet)\n",
        "prophet_rmse = np.sqrt(mean_squared_error(y_true, y_pred_prophet))\n",
        "\n",
        "print(\"🔍 Forecast Performance Comparison:\")\n",
        "print(f\"📈 Prophet         - MAE: {prophet_mae:.3f}, RMSE: {prophet_rmse:.3f}\")\n",
        "print(f\"📉 Naïve           - MAE: {naive_mae:.3f}, RMSE: {naive_rmse:.3f}\")\n",
        "print(f\"🕒 Seasonal Naïve  - MAE: {seasonal_mae:.3f}, RMSE: {seasonal_rmse:.3f}\")"
      ],
      "metadata": {
        "id": "VQJBan9KJJDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting With SARIMAX MODEL"
      ],
      "metadata": {
        "id": "VT_AvH-3Jv4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels --quiet"
      ],
      "metadata": {
        "id": "5LnVl3XJJnbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "H1D5Of47KyBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Make a fresh copy to work on\n",
        "df_sarimax = df.copy()\n",
        "\n",
        "# Ensure datetime index\n",
        "df_sarimax.index = pd.to_datetime(df_sarimax.index)\n",
        "\n",
        "# Convert boolean features to int\n",
        "df_sarimax['ev_owner'] = df_sarimax['ev_owner'].astype(int)\n",
        "df_sarimax['solar_installed'] = df_sarimax['solar_installed'].astype(int)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df_sarimax = pd.get_dummies(df_sarimax, columns=['region', 'property_type'], drop_first=True)\n",
        "\n",
        "# Now convert any remaining bools (from one-hot encoding) to int\n",
        "for col in df_sarimax.columns:\n",
        "    if df_sarimax[col].dtype == bool:\n",
        "        df_sarimax[col] = df_sarimax[col].astype(int)\n",
        "\n",
        "# Drop non-numeric / irrelevant features\n",
        "df_sarimax = df_sarimax.drop(columns=['date', 'weekday', 'month'])\n",
        "\n",
        "# Sort by time\n",
        "df_sarimax = df_sarimax.sort_index()\n",
        "\n",
        "# ✅ Show updated dtypes to confirm\n",
        "print(df_sarimax.dtypes)\n",
        "\n",
        "# Preview the cleaned DataFrame\n",
        "df_sarimax.head()"
      ],
      "metadata": {
        "id": "WirY5jqwK4bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data for SARIMAX Modeling\n",
        "\n",
        "This cell prepares a clean and numeric dataset suitable for use with the **SARIMAX** model:\n",
        "\n",
        "- `df_sarimax = df.copy()`: Starts with a fresh copy of the original data to avoid altering it.\n",
        "- `df_sarimax.index = pd.to_datetime(...)`: Ensures the DataFrame index is in datetime format, which is required for time series modeling.\n",
        "\n",
        "**Feature preprocessing:**\n",
        "- Boolean features like `ev_owner` and `solar_installed` are converted to integers (0 or 1).\n",
        "- Categorical variables (`region` and `property_type`) are one-hot encoded using `pd.get_dummies`, with `drop_first=True` to avoid multicollinearity.\n",
        "\n",
        "**Additional cleaning:**\n",
        "- Any leftover boolean columns (possibly from encoding) are also converted to integers.\n",
        "- Drops the `date` column since the timestamp is already captured in the index.\n",
        "- Sorts the DataFrame chronologically to preserve time order.\n",
        "\n",
        "Finally:\n",
        "- Prints the data types to confirm everything is numeric.\n",
        "- Shows the top rows of the cleaned dataset to preview the structure before modeling.\n",
        "\n",
        "This preprocessing ensures that SARIMAX can handle all features correctly during fitting."
      ],
      "metadata": {
        "id": "Yp_34QH1LM9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the target column\n",
        "target = 'power_consumption_kwh'\n",
        "\n",
        "# Drop unwanted columns from exogenous features\n",
        "exog_cols = df_sarimax.columns.drop([target, 'voltage', 'current'], errors='ignore')\n",
        "\n",
        "# Define train-test split\n",
        "split_index = int(len(df_sarimax) * 0.8)\n",
        "train_end = df_sarimax.index[split_index]\n",
        "\n",
        "# Create training and testing sets\n",
        "train_y = df_sarimax[target].loc[:train_end]\n",
        "test_y = df_sarimax[target].loc[train_end:]\n",
        "\n",
        "train_X = df_sarimax[exog_cols].loc[:train_end]\n",
        "test_X = df_sarimax[exog_cols].loc[train_end:]"
      ],
      "metadata": {
        "id": "hz8wVHTeLE_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Target, Exogenous Variables, and Time-Based Train-Test Split for SARIMAX\n",
        "target = 'power_consumption_kwh': Specifies the target variable to forecast.\n",
        "exog_cols = df_sarimax.columns.drop(target): Defines all other columns as exogenous variables (external predictors), which SARIMAX can use to improve forecasting accuracy.\n",
        "Train-test split:\n",
        "\n",
        "split_index: Computes the index that corresponds to 80% of the dataset length.\n",
        "\n",
        "train_end = df_sarimax.index[split_index]: Gets the actual timestamp at the split point.\n",
        "\n",
        "train_y and test_y: Contain the target values for the training and testing periods, respectively.\n",
        "\n",
        "train_X and test_X: Contain the exogenous features for the corresponding time ranges.\n",
        "\n",
        "This setup ensures that both the target and predictor variables are properly aligned and time-ordered for training and evaluating the SARIMAX model."
      ],
      "metadata": {
        "id": "LQYWZAVrL4Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y.dtypes)\n",
        "print(train_X.dtypes)"
      ],
      "metadata": {
        "id": "9ZQMz0VcLWdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Fit SARIMAX model\n",
        "sarimax_model = SARIMAX(train_y, exog=train_X, order=(1, 0, 0), seasonal_order=(1, 0, 0, 48), enforce_stationarity=False, enforce_invertibility=False)\n",
        "sarimax_results = sarimax_model.fit(disp=False)\n",
        "\n",
        "print(\"✅ SARIMAX model fitted.\")"
      ],
      "metadata": {
        "id": "3CC0ruiQMJe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the SARIMAX Model\n",
        "\n",
        "- `SARIMAX(...)`: Initializes a **Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors** model with the following parameters:\n",
        "  - `order=(1, 0, 0)`: Standard ARIMA order:\n",
        "    - AR (p=1): One lag of the dependent variable\n",
        "    - I (d=0): No differencing (assumes stationarity)\n",
        "    - MA (q=0): No moving average component\n",
        "  - `seasonal_order=(1, 0, 0, 48)`: Seasonal component:\n",
        "    - Seasonal AR with 1 lag\n",
        "    - Seasonal period = 48 (assumes 30-minute data, so 48 steps = 1 day)\n",
        "  - `exog=train_X`: Includes external features (regressors) during training\n",
        "  - `enforce_stationarity=False`, `enforce_invertibility=False`: These relax constraints during estimation to allow more flexible model fitting.\n",
        "\n",
        "- `sarimax_model.fit(disp=False)`: Fits the model to the training target (`train_y`) and its exogenous inputs, suppressing output display.\n",
        "\n",
        "Once fitted, the model learns both short-term and seasonal patterns in the target series while accounting for influence from external variables."
      ],
      "metadata": {
        "id": "iYxkwArbRofo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using SARIMAX\n",
        "sarimax_forecast = sarimax_results.predict(start=len(train_y), end=len(train_y) + len(test_y) - 1, exog=test_X)"
      ],
      "metadata": {
        "id": "wyMmnVCzRpLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Forecasts with the SARIMAX Model\n",
        "\n",
        "- `sarimax_results.predict(...)`: Generates predictions using the trained SARIMAX model on the **test period**.\n",
        "\n",
        "Parameters:\n",
        "- `start=len(train_y)`: Specifies the starting index for forecasting—immediately after the training data ends.\n",
        "- `end=len(train_y) + len(test_y) - 1`: Forecasts up to the length of the test set.\n",
        "- `exog=test_X`: Supplies the corresponding exogenous features for the forecast period.\n",
        "\n",
        "This step produces a time-aligned forecast of power consumption, incorporating both past values and external factors like region, hour, and solar installation status."
      ],
      "metadata": {
        "id": "onC2rkEwRy0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate metrics\n",
        "sarimax_mae = mean_absolute_error(test_y, sarimax_forecast)\n",
        "sarimax_rmse = np.sqrt(mean_squared_error(test_y, sarimax_forecast))\n",
        "\n",
        "print(f\"📊 SARIMAX Evaluation Results:\")\n",
        "print(f\"🔹 MAE:  {mae:.3f} kWh\")\n",
        "print(f\"🔹 RMSE: {rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "bX4OqPZxRuDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM MODEL"
      ],
      "metadata": {
        "id": "4ie_CJslR_YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy to preserve the original\n",
        "df_lstm = df.copy()\n",
        "\n",
        "# Ensure timestamp is the index (for time series)\n",
        "if df_lstm.index.name != 'datetime':\n",
        "    df_lstm = df_lstm.set_index('datetime')\n",
        "\n",
        "# Encode Categorical and boolean values\n",
        "# Convert boolean columns to integers\n",
        "df_lstm['ev_owner'] = df_lstm['ev_owner'].astype(int)\n",
        "df_lstm['solar_installed'] = df_lstm['solar_installed'].astype(int)\n",
        "\n",
        "# One-hot encode region and property_type\n",
        "region_dummies = pd.get_dummies(df_lstm['region'], prefix='region')\n",
        "property_dummies = pd.get_dummies(df_lstm['property_type'], prefix='property')\n",
        "\n",
        "# Concatenate with the original DataFrame\n",
        "df_lstm = pd.concat([df_lstm, region_dummies, property_dummies], axis=1)"
      ],
      "metadata": {
        "id": "ZM_4VYbKR4Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cyclical features of hour day\n",
        "\n",
        "# If not already present, create hour_of_day\n",
        "df_lstm['hour_of_day'] = df_lstm.index.hour\n",
        "\n",
        "# Create sine and cosine transforms to capture daily cycles\n",
        "df_lstm['hour_sin'] = np.sin(2 * np.pi * df_lstm['hour_of_day'] / 24)\n",
        "df_lstm['hour_cos'] = np.cos(2 * np.pi * df_lstm['hour_of_day'] / 24)"
      ],
      "metadata": {
        "id": "lmsiGgwRSh_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target\n",
        "feature_cols = [\n",
        "    'temperature_c',\n",
        "    'ev_owner', 'solar_installed',\n",
        "    'hour_sin', 'hour_cos',\n",
        "    'region_east', 'region_north', 'region_south', 'region_west',\n",
        "    'property_commercial', 'property_residential'\n",
        "]\n",
        "target_col = 'power_consumption_kwh'\n",
        "\n",
        "# Drop NA values (if any)\n",
        "df_lstm = df_lstm.dropna(subset=feature_cols + [target_col])"
      ],
      "metadata": {
        "id": "cL_u7AMUSxvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features\n",
        "X = df_lstm[feature_cols].values\n",
        "# Target\n",
        "y = df_lstm[target_col].values\n",
        "\n",
        "# Scale features and target\n",
        "scaler_X = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "VQRzveuzSxa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LSTM Sequences\n",
        "def create_sequences(X, y, seq_length):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_length):\n",
        "        Xs.append(X[i:i+seq_length])\n",
        "        ys.append(y[i+seq_length])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "seq_length = 24  # Use last 24 intervals (e.g., 1 day if hourly) to predict next step\n",
        "\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
        "\n",
        "# Train/test split (80/20)\n",
        "split_idx = int(len(X_seq) * 0.8)\n",
        "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]"
      ],
      "metadata": {
        "id": "zlvdREgkS6vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train LSTM\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(seq_length, X_train.shape[2]), return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)"
      ],
      "metadata": {
        "id": "v4BVKGFYTHRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "y_test_inv = scaler_y.inverse_transform(y_test)\n",
        "\n",
        "lstm_mae = mean_absolute_error(y_test_inv, y_pred)\n",
        "lstm_rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred))\n",
        "\n",
        "print(f\"LSTM MAE: {mae:.3f} kWh\")\n",
        "print(f\"LSTM RMSE: {rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "kZoC6VywTQCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Predictions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(y_test_inv, label='Actual')\n",
        "plt.plot(y_pred, label='Predicted')\n",
        "plt.title('LSTM: Actual vs Predicted Power Consumption')\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('kWh')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cQoYO46lTmXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### USING TRADITIONAL RANDOM FOREST MODEL"
      ],
      "metadata": {
        "id": "SArgcTK6UPCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make a copy# Assume df is your original DataFrame with datetime index\n",
        "df_model = df.copy()\n",
        "\n",
        "# Confirm the datetime index is set\n",
        "df_model.index = pd.to_datetime(df_model.index)"
      ],
      "metadata": {
        "id": "t7JICb7zT76W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model['lag_30min'] = df_model['power_consumption_kwh'].shift(1)\n",
        "df_model['lag_1h'] = df_model['power_consumption_kwh'].shift(2)  # 1 hour = 2 x 30 mins"
      ],
      "metadata": {
        "id": "1qMX-LpsUi5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model['rolling_avg_1h'] = df_model['power_consumption_kwh'].rolling(2).mean()\n",
        "df_model['rolling_avg_2h'] = df_model['power_consumption_kwh'].rolling(4).mean()"
      ],
      "metadata": {
        "id": "r9n3uhFMU-7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'hour_of_day' and 'date' already exist\n",
        "df_model['is_weekend'] = df_model.index.weekday >= 5  # Saturday=5, Sunday=6\n",
        "\n",
        "# Sine and cosine encoding for cyclical hour\n",
        "df_model['hour_sin'] = np.sin(2 * np.pi * df_model['hour_of_day'] / 24)\n",
        "df_model['hour_cos'] = np.cos(2 * np.pi * df_model['hour_of_day'] / 24)\n"
      ],
      "metadata": {
        "id": "WRS1UPbdVCth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model = pd.get_dummies(df_model, columns=['property_type', 'region'], drop_first=False)"
      ],
      "metadata": {
        "id": "itzdLTtaVIbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model = df_model.dropna()"
      ],
      "metadata": {
        "id": "Zmb1J6K7VQy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'power_consumption_kwh'\n",
        "\n",
        "features = [\n",
        "    'lag_30min', 'lag_1h',\n",
        "    'rolling_avg_1h', 'rolling_avg_2h',\n",
        "    'hour_of_day', 'is_weekend',\n",
        "    'hour_sin', 'hour_cos',\n",
        "    'temperature_c', 'ev_owner', 'solar_installed',\n",
        "    'property_type_commercial', 'property_type_residential',\n",
        "    'region_north', 'region_south', 'region_east', 'region_west'\n",
        "]\n",
        "\n",
        "X = df_model[features]\n",
        "y = df_model[target]"
      ],
      "metadata": {
        "id": "FH-bcBvPVUkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(len(df_model) * 0.8)\n",
        "\n",
        "X_train = X.iloc[:split_index]\n",
        "y_train = y.iloc[:split_index]\n",
        "\n",
        "X_test = X.iloc[split_index:]\n",
        "y_test = y.iloc[split_index:]"
      ],
      "metadata": {
        "id": "1RHAY_phVW1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RX72ed3LVah3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rf_mae = mean_absolute_error(y_test, y_pred)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Manual RMSE\n",
        "\n",
        "print(f\"MAE: {mae:.3f} kWh\")\n",
        "print(f\"RMSE: {rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "BwytwoMEVdHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reset index to align on the same axis if needed (especially if datetime index)\n",
        "y_test_aligned = y_test.reset_index(drop=True)\n",
        "y_pred_aligned = pd.Series(y_pred, index=y_test_aligned.index)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(y_test_aligned, label=\"Actual\", color='blue', linewidth=2)\n",
        "plt.plot(y_pred_aligned, label=\"Predicted\", color='tomato')\n",
        "plt.title(\"Actual vs Predicted Power Consumption (kWh)\")\n",
        "plt.xlabel(\"Sample (Time Progression)\")\n",
        "plt.ylabel(\"Power Consumption (kWh)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a6t6FuYtW-Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using XGBOOST Model"
      ],
      "metadata": {
        "id": "4ErC1zj6Xrg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "EuMYF-gfXEba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "xgboost_mae = mean_absolute_error(y_test, y_pred)\n",
        "xgboost_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"XGBoost MAE: {mae:.3f} kWh\")\n",
        "print(f\"XGBoost RMSE: {rmse:.3f} kWh\")"
      ],
      "metadata": {
        "id": "aQNjuSL2Xwrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🔍 Forecast Performance Comparison:\")\n",
        "print(f\"📈 Prophet         - MAE: {prophet_mae:.3f}, RMSE: {prophet_rmse:.3f}\")\n",
        "print(f\"📉 Naïve           - MAE: {naive_mae:.3f}, RMSE: {naive_rmse:.3f}\")\n",
        "print(f\"🕒 Seasonal Naïve  - MAE: {seasonal_mae:.3f}, RMSE: {seasonal_rmse:.3f}\")\n",
        "print(f\"📊 SARIMAX          - MAE: {sarimax_mae:.3f}, RMSE: {sarimax_rmse:.3f}\")\n",
        "print(f\"🌳 LSTM             - MAE: {lstm_mae:.3f}, RMSE: {lstm_rmse:.3f}\")\n",
        "print(f\"🌳 Random Forest   - MAE: {rf_mae:.3f}, RMSE: {rf_rmse:.3f}\")\n",
        "print(f\"📊 XGBoost          - MAE: {xgboost_mae:.3f}, RMSE: {xgboost_rmse:.3f}\")\n"
      ],
      "metadata": {
        "id": "iUBhcUbUX0RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VJwk7q9-YnTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}