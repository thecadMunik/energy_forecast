{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ENERGY REAL-TIME FORECASTING PROJECT"
      ],
      "metadata": {
        "id": "yr6aq3j5d65b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages in Colab\n",
        "!pip install pandas  matplotlib seaborn scikit-learn xgboost requests"
      ],
      "metadata": {
        "id": "Q4BtUmcp0TLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading from URL SupaBase\n",
        "\n",
        "1. **Imports**  \n",
        "   - `pandas` for handling tables of data.  \n",
        "   - `requests` for making HTTP calls to the Supabase API.\n",
        "\n",
        "2. **Configuration**  \n",
        "   - `SUPABASE_URL` and `SUPABASE_API_KEY` point to your database and authorize access.  \n",
        "   - `TABLE` is the name of the table you’ll pull (`smart_meter_readings`).\n",
        "\n",
        "3. **`load_data()` function**  \n",
        "   - Builds the URL to fetch **all columns** (`select=*`) from your table, sorted by `timestamp` ascending (`order=timestamp.asc`).  \n",
        "   - Sends a GET request with your API key.  \n",
        "   - If successful (`status_code == 200`), prints the number of records retrieved and converts the JSON response into a pandas DataFrame.  \n",
        "   - If unsuccessful, raises an error with the status code and message.\n",
        "\n",
        "4. **Usage**  \n",
        "   - Calls `load_data()` to populate `df` with the full dataset.  \n",
        "   - Displays the first few rows of `df` using `df.head()`.  \n",
        "\n",
        "> **Purpose:** Pull the entire smart-meter readings table into your notebook for analysis."
      ],
      "metadata": {
        "id": "iwi18MFAfEmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "SUPABASE_URL     = \"https://qpnzblvhwgmzorcdduuy.supabase.co\"\n",
        "SUPABASE_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFwbnpibHZod2dtem9yY2RkdXV5Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTEyNTcyNDEsImV4cCI6MjA2NjgzMzI0MX0._q7_v3XX-_tqKiFRI4KDy4e7IX5GIkDwqPSlU78FQCg\"\n",
        "TABLE            = \"smart_meter_readings\"\n",
        "\n",
        "def load_data():\n",
        "    url = (\n",
        "        f\"{SUPABASE_URL}/rest/v1/{TABLE}\"\n",
        "        \"?select=*\"               # fetch all columns\n",
        "        \"&order=timestamp.asc\"    # order by timestamp ascending\n",
        "    )\n",
        "    headers = {\n",
        "        \"apikey\": SUPABASE_API_KEY,\n",
        "        \"Authorization\": f\"Bearer {SUPABASE_API_KEY}\"\n",
        "    }\n",
        "\n",
        "    res = requests.get(url, headers=headers)\n",
        "    if res.status_code == 200:\n",
        "        print(\"✅ Data pulled successfully: \", len(res.json()), \"records\\n\")\n",
        "        return pd.DataFrame(res.json())\n",
        "    else:\n",
        "        raise Exception(f\"❌ Error: {res.status_code}\\n{res.text}\")\n",
        "\n",
        "# usage\n",
        "df = load_data()\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "qrZ_05IIe1eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Inspection"
      ],
      "metadata": {
        "id": "JqzWh3QPgRJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "7ULuiSDnflJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 668 rows and 13 columns in the dataset"
      ],
      "metadata": {
        "id": "LvyhqgCmgxbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking out the column names\n",
        "\n",
        "for col in df.columns:\n",
        "    print(col)"
      ],
      "metadata": {
        "id": "rFh-PExAgZQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at the various datatypes for each features\n",
        "df.info()"
      ],
      "metadata": {
        "id": "J5-gAVjYg4Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "VkAStH8ohoyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Convert the 'timestamp' column from Unix seconds to a readable datetime format and store it in a new 'datetime' column\n",
        "\n",
        "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')"
      ],
      "metadata": {
        "id": "9Z5Ni4VWhaas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract date and ISO week number and store in new columns\n",
        "\n",
        "df['date'] = df['datetime'].dt.date\n",
        "df['week'] = df['datetime'].dt.isocalendar().week"
      ],
      "metadata": {
        "id": "eo2n82oghyhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Convert date to datetime, week to integer\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['week'] = df['week'].astype(int)"
      ],
      "metadata": {
        "id": "jjSizQRHkU_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6SS3b_Rni2Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "IBMRUVA7i9oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the date range and the number of days and unique ISO weeks in the dataset\n",
        "min_date = df['date'].min()\n",
        "max_date = df['date'].max()\n",
        "days     = (max_date - min_date).days + 1\n",
        "weeks    = df['week'].nunique()\n",
        "\n",
        "print(f\"Dataset covers {days} days, from {min_date} to {max_date} → {weeks} ISO weeks\\n\")\n"
      ],
      "metadata": {
        "id": "AAY3DWfUjFrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of readings for each date and display them sorted by date\n",
        "daily_counts = df['date'].value_counts().sort_index()\n",
        "print(\"Readings per day:\\n\", daily_counts)"
      ],
      "metadata": {
        "id": "iEmeIhjkjiBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Check for nulls and duplicates\n",
        "print(\"Nulls per column:\\n\", df.isnull().sum(), \"\\n\")\n",
        "print(\"Duplicates:\", df.duplicated().sum())"
      ],
      "metadata": {
        "id": "R8PyAXRQj91l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's no null or duplicate values in the dataset"
      ],
      "metadata": {
        "id": "KGyx2ItYkzdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Drop irrelevant columns\n",
        "df = df.drop(columns=['id', 'timestamp'])"
      ],
      "metadata": {
        "id": "sLoeg1Vhnp9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move 'datetime' to the index which automatically sorts the data in datetime order for time series analysis\n",
        "df = df.set_index('datetime').sort_index()"
      ],
      "metadata": {
        "id": "hZPrbK3coa-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Hdf2S8EloNpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round down all datetime index values to the nearest second\n",
        "df.index = df.index.floor('S')\n"
      ],
      "metadata": {
        "id": "i8wwxgxfoV4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "9oIMGzLXopqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization and Analysis"
      ],
      "metadata": {
        "id": "0pUWThi0oy0M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSpKECRCosRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}